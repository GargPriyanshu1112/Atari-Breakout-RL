Loss function impl
See how to use loss function in update function.


get_gradients impl
update_weights impl
update function impl

"""
def loss_fn(self, action_probs, actions, targets):
        # print(f"action_probs.shape: {action_probs.shape}")
        estimated_qvals = tf.reduce_sum(
            tf.multiply(action_probs, tf.one_hot(actions, depth=self.num_actions)),
            axis=1,
        )
        # print(f"estimated_qvals.shape: {estimated_qvals.shape}")
        # print(targets, estimated_qvals)
        loss = self.huber_loss(targets, estimated_qvals)
        # print(f"loss.shape: {loss.shape}")
        # print(f"loss: {loss}")
        return loss

    def update(self, states, actions, targets):
        with tf.GradientTape() as t:
            action_probs = self.predict(states)
            loss = self.loss_fn(action_probs, actions, targets)
        self.optimizer.minimize(
            loss=loss, var_list=self.model.trainable_weights, tape=t
        )
        return loss
"""

"""
    @tf.function
    def train_step(model, x, y_true):
        with tf.GradientTape() as t:
            # Make predictions
            y_pred = model(x)
            # Calculate loss
            loss_value = loss_fn(y_true, y_pred)
        
        # Derive gradients
        gradients = t.gradient(loss_value, model.trainable_weights)
        # Apply gradients
        optimizer.apply_gradients(zip(gradients, model.trainable_weights))    

        return loss_value
"""

"""
# def get_gradients(self):
#         def update(self, states, actions, targets):
#         with tf.GradientTape() as t:
#             action_probs = self.predict(states)
#             loss = self.loss_fn(action_probs, actions, targets)
#         self.optimizer.minimize(
#             loss=loss, var_list=self.model.trainable_weights, tape=t
#         )
#         return loss
"""


"""
Certainly! Here's a concise summary:
1.	Deep Q Learning-based Atari Breakout Program:
o	Developed a deep reinforcement learning model using the Deep Q Learning algorithm for Atari Breakout, achieving [mention any significant accomplishments]. Implemented in Python using TensorFlow/PyTorch and OpenAI Gym.
2.	Asynchronous Advantage Actor-Critic (A3C) Implementation:
o	Designed and implemented an A3C algorithm for [specify the application], showcasing parallelized training for efficiency. Achieved [mention any notable achievements] using Python, TensorFlow/PyTorch, and OpenAI Gym.

Project Title: Deep Q Learning-based Atari Breakout Program
•	Description: Developed a deep reinforcement learning model using the Deep Q Learning algorithm to play the Atari Breakout game. Implemented a neural network to approximate the Q-function and utilized experience replay for more stable learning. Achieved [mention any significant accomplishments, like achieving high scores or outperforming benchmarks].
Led the development of a sophisticated deep reinforcement learning model utilizing the Deep Q Learning algorithm for mastery in the Atari Breakout game. Engineered a neural network to approximate the Q-function and strategically implemented an experience replay buffer, enhancing stability in learning.
Project Title: Asynchronous Advantage Actor-Critic (A3C) Implementation
•	Description: Designed and implemented an Asynchronous Advantage Actor-Critic (A3C) algorithm for [specify the application or environment, e.g., game playing, robotic control, etc.]. Utilized parallelized training to improve efficiency and achieve faster convergence. Demonstrated the effectiveness of the A3C algorithm in [mention any notable achievements or improvements].

Currently implementing Asynchronous Advantage Actor-Critic (A3C) algorithm to parallelize training and improve faster convergence.
Utilized Deep Q Learing algorithm for creating the agent that learns the Atari Breakout game. Implemented an experience replay buffer, enhancing stability in learning
.

"""



"""
Implemented Deep Q-Learning (DQL) in a reinforcement learning project, utilizing a neural network for Q-value function approximation. Incorporated experience replay for training stability and a target network to prevent chasing a moving target. Applied an epsilon-greedy policy for balanced exploration and exploitation during action selection.
Implementing Asynchronous Advantage Actor-Critic (A3C) algorithm to parallelize training for accelerated convergence.


"""
Implemented Deep Q-Learning, utilizing a neural network for Q-value function approximation. 
Incorporated experience replay buffer for training stability and a target network to prevent chasing a moving target. Applied an epsilon-greedy policy for balanced exploration and exploitation during action selection.
Currently implementing Asynchronous Advantage Actor-Critic (A3C) algorithm to parallelize training for accelerated convergence.
"""


"""
The actor-network is responsible for determining the actions that the agent should take, while the critic network is responsible for evaluating the quality of those actions.

The actor tries to maximize the expected return of the policy, and the critic network tries to minimize the error between the estimated return and the actual return.

Here we created an Actor-network by deriving from keras.module class. Our Actor-network holds three dense layers. The last one is used to output the Action values. The Actor-network takes state value as input and outputs the respective action for the given state.

The critic network is created by deriving from keras.module class. Our Critic network holds three dense layers. The last one is used to output the Q-value. The critic network takes the state value as input and outputs the respective Q-value.
"""